{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f718975e",
   "metadata": {},
   "source": [
    "# ðŸ§  Spooky Author Identification\n",
    "\n",
    "## Objective:\n",
    "###### Predict the author of horror story excerpts written by Edgar Allan Poe, Mary Shelley, or H.P. Lovecraft.\n",
    "\n",
    "###### The dataset consists of text samples extracted from public domain works by these three iconic authors. To create the dataset, longer texts were segmented into smaller excerptsâ€”primarily sentencesâ€”using the MaxEnt sentence tokenizer from CoreNLP. As a result, some fragments may not be complete sentences.\n",
    "\n",
    "###### Your goal is to build a model that can accurately determine which author wrote each excerpt in the test set.\n",
    "\n",
    "###### Dataset Files:\n",
    "\n",
    "###### train.csv â€” contains labeled training excerpts with corresponding author labels (EAP, HPL, MWS)\n",
    "\n",
    "###### test.csv â€” contains unlabeled excerpts for which author predictions are required\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5eae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from nltk import pos_tag, bigrams, FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81606bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokens'] = train['text'].str.lower().apply(lambda x: TextBlob(x).words)\n",
    "train['nb_tokens'] = train['tokens'].apply(len)\n",
    "train['word_length'] = train['tokens'].apply(lambda x: np.mean([len(word) for word in x]))\n",
    "train['sentences'] = train['text'].apply(lambda x: TextBlob(x).sentences)\n",
    "train['nb_sentences'] = train['sentences'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870725bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = train.groupby('author')['nb_tokens'].agg(['sum', 'count'])\n",
    "grouped['mean_words'] = grouped['sum'] / grouped['count']\n",
    "\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "grouped['count'].plot(kind='bar', ax=axarr[0], title='Number of extracts per author')\n",
    "grouped['mean_words'].plot(kind='bar', ax=axarr[1], title='Mean amount of words per author')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ffd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['one', \"'s\"])\n",
    "train['useful'] = train['tokens'].apply(lambda tokens: [w for w in tokens if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96323a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def translate_tag_pos(tup):\n",
    "    tag_map = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    for key in tag_map:\n",
    "        if tup[1].startswith(key):\n",
    "            return (tup[0], tag_map[key])\n",
    "    return None\n",
    "\n",
    "def lemmatize_with_new_tags(tags):\n",
    "    lemmas = []\n",
    "    for t in tags:\n",
    "        new_tag = translate_tag_pos(t)\n",
    "        if new_tag:\n",
    "            lemmas.append(wordnet_lemmatizer.lemmatize(new_tag[0], pos=new_tag[1]))\n",
    "        else:\n",
    "            lemmas.append(t[0])\n",
    "    return lemmas\n",
    "\n",
    "train['tags'] = train['useful'].apply(pos_tag)\n",
    "train['lemma'] = train['tags'].apply(lemmatize_with_new_tags)\n",
    "train['vocab_wealth'] = train.apply(lambda row: len(set(row['lemma'])) / len(row['tokens']) * 100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d153ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "EAP = sum(train[train.author == \"EAP\"]['lemma'].tolist(), [])\n",
    "HPL = sum(train[train.author == \"HPL\"]['lemma'].tolist(), [])\n",
    "MWS = sum(train[train.author == \"MWS\"]['lemma'].tolist(), [])\n",
    "\n",
    "hpl_freq = FreqDist(HPL)\n",
    "poe_freq = FreqDist(EAP)\n",
    "mws_freq = FreqDist(MWS)\n",
    "\n",
    "hpl_bg_freq = FreqDist(bigrams(HPL))\n",
    "poe_bg_freq = FreqDist(bigrams(EAP))\n",
    "mws_bg_freq = FreqDist(bigrams(MWS))\n",
    "\n",
    "def compute_author_features(row):\n",
    "    lemma = row['lemma']\n",
    "    bg = list(bigrams(lemma))\n",
    "    n = len(lemma)\n",
    "    nb = len(bg)\n",
    "\n",
    "    return pd.Series({\n",
    "        'hpl_word_count': sum(hpl_freq[w] for w in lemma) / n if n else 0,\n",
    "        'poe_word_count': sum(poe_freq[w] for w in lemma) / n if n else 0,\n",
    "        'mws_word_count': sum(mws_freq[w] for w in lemma) / n if n else 0,\n",
    "        'hpl_bigram_count': sum(hpl_bg_freq[b] for b in bg) / nb if nb else 0,\n",
    "        'poe_bigram_count': sum(poe_bg_freq[b] for b in bg) / nb if nb else 0,\n",
    "        'mws_bigram_count': sum(mws_bg_freq[b] for b in bg) / nb if nb else 0,\n",
    "    })\n",
    "\n",
    "train = pd.concat([train, train.apply(compute_author_features, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e9a39-eb2b-495e-9ca9-76b7548b05b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b719c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train['target'] = encoder.fit_transform(train['author'])\n",
    "\n",
    "features = ['nb_tokens', 'word_length', 'nb_sentences', 'vocab_wealth',\n",
    "            'hpl_word_count', 'poe_word_count', 'mws_word_count',\n",
    "            'hpl_bigram_count', 'poe_bigram_count', 'mws_bigram_count']\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(train[features], train['target'], test_size=0.25)\n",
    "rfc = RandomForestClassifier(n_estimators=800, max_depth=20, max_features='sqrt')\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "prediction = rfc.predict(x_validate)\n",
    "validate = train.loc[x_validate.index].copy()\n",
    "validate['target_predict'] = prediction\n",
    "validate['predicted_author'] = encoder.inverse_transform(validate['target_predict'])\n",
    "\n",
    "print(classification_report(validate['target'], validate['target_predict'], target_names=encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
